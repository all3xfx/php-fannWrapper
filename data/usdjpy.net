FANN_FLO_2.1
num_layers=4
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=6 11 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (6, 11, 5.00000000000000000000e-01) (6, 11, 5.00000000000000000000e-01) (6, 11, 5.00000000000000000000e-01) (6, 11, 5.00000000000000000000e-01) (6, 11, 5.00000000000000000000e-01) (6, 11, 5.00000000000000000000e-01) (6, 11, 5.00000000000000000000e-01) (6, 11, 5.00000000000000000000e-01) (6, 11, 5.00000000000000000000e-01) (6, 11, 5.00000000000000000000e-01) (0, 11, 0.00000000000000000000e+00) (11, 11, 5.00000000000000000000e-01) (11, 11, 5.00000000000000000000e-01) (11, 11, 5.00000000000000000000e-01) (0, 11, 0.00000000000000000000e+00) (4, 11, 5.00000000000000000000e-01) (0, 11, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 7.11909011006355285645e-02) (1, -5.62581606209278106689e-02) (2, 8.28612446784973144531e-02) (3, -5.37037011235952377319e-03) (4, 4.87914076074957847595e-03) (5, -9.51771512627601623535e-02) (0, 5.70468418300151824951e-02) (1, -2.81990016810595989227e-03) (2, -3.18740606307983398438e-02) (3, 7.68020823597908020020e-02) (4, 7.05832540988922119141e-02) (5, 8.45423564314842224121e-02) (0, 9.61964279413223266602e-02) (1, -3.87669689953327178955e-02) (2, 4.92983236908912658691e-02) (3, 3.77359241247177124023e-02) (4, -6.85819834470748901367e-02) (5, -8.46953690052032470703e-02) (0, 6.28776326775550842285e-02) (1, -5.56783005595207214355e-02) (2, 8.10405164957046508789e-02) (3, 7.00208172202110290527e-02) (4, 5.03862872719764709473e-02) (5, 7.73970643058419227600e-03) (0, -5.19082248210906982422e-02) (1, 3.53811308741569519043e-02) (2, -1.23151373118162155151e-02) (3, -9.38974618911743164062e-02) (4, -4.57009598612785339355e-02) (5, -3.47559228539466857910e-02) (0, 6.40950873494148254395e-02) (1, 3.60490102320909500122e-03) (2, 4.68111336231231689453e-02) (3, 5.94595894217491149902e-02) (4, -5.56950867176055908203e-02) (5, 5.53987808525562286377e-02) (0, 8.25965330004692077637e-02) (1, 9.43789631128311157227e-02) (2, -5.92875368893146514893e-02) (3, 9.27921310067176818848e-02) (4, 1.52887748554348945618e-02) (5, 9.55231264233589172363e-02) (0, 9.13750305771827697754e-02) (1, 8.43521431088447570801e-02) (2, -7.82467126846313476562e-02) (3, 4.85613793134689331055e-02) (4, 7.33090788125991821289e-02) (5, -2.29775086045265197754e-02) (0, -4.50395531952381134033e-02) (1, -1.74033530056476593018e-02) (2, 8.65717381238937377930e-02) (3, 7.14150369167327880859e-02) (4, -1.80438924580812454224e-02) (5, 3.68319451808929443359e-02) (0, 3.10025662183761596680e-02) (1, -7.02066570520401000977e-02) (2, -1.67185720056295394897e-02) (3, 3.52709963917732238770e-02) (4, 4.80002276599407196045e-02) (5, -2.79787722975015640259e-02) (6, -8.94098207354545593262e-02) (7, -6.11737854778766632080e-02) (8, 7.12879225611686706543e-02) (9, -1.75427133217453956604e-03) (10, -9.87825244665145874023e-02) (11, -5.40192872285842895508e-02) (12, 2.45224945247173309326e-02) (13, -7.42787644267082214355e-02) (14, -1.86553853563964366913e-03) (15, -3.49754802882671356201e-02) (16, -1.05974078178405761719e-01) (6, -8.75710844993591308594e-02) (7, -4.97030541300773620605e-02) (8, 5.31809329986572265625e-02) (9, 4.00979444384574890137e-02) (10, -8.66479650139808654785e-02) (11, 3.17696444690227508545e-02) (12, 6.56457850709557533264e-03) (13, 2.31293477118015289307e-02) (14, -5.52699528634548187256e-02) (15, 6.80098757147789001465e-02) (16, 9.11190658807754516602e-02) (6, -1.86783552635461091995e-03) (7, -4.50481064617633819580e-02) (8, -9.83953475952148437500e-03) (9, 2.48515419661998748779e-02) (10, 6.78801313042640686035e-02) (11, 1.74542590975761413574e-02) (12, 8.07862356305122375488e-03) (13, 1.14435693249106407166e-02) (14, -7.43556767702102661133e-02) (15, -7.72681236267089843750e-02) (16, -4.20838706195354461670e-02) (17, 9.97974202036857604980e-02) (18, -1.20861940085887908936e-01) (19, 7.90179818868637084961e-02) (20, -1.91772544384002685547e+00) 
